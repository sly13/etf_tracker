#!/usr/bin/env python3
"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –±–µ–∑ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ ML.
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
from typing import Tuple, Optional
import joblib
from dotenv import load_dotenv

# ML libraries
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
import xgboost as xgb

# Local imports
from train import FeatureEngineer, BitcoinPredictor

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()


def create_synthetic_data(days: int = 365) -> pd.DataFrame:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏.
    
    Args:
        days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–∞–Ω–Ω—ã—Ö
        
    Returns:
        DataFrame —Å —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    """
    logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ {days} –¥–Ω–µ–π...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞
    dates = pd.date_range(start='2023-01-01', periods=days, freq='D')
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    base_price = 45000
    daily_volatility = 0.02
    trend = 0.0005  # –ù–µ–±–æ–ª—å—à–æ–π –≤–æ—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–µ–Ω–¥
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ü–µ–Ω
    prices = [base_price]
    for i in range(1, len(dates)):
        # –°–ª—É—á–∞–π–Ω–æ–µ –±–ª—É–∂–¥–∞–Ω–∏–µ —Å —Ç—Ä–µ–Ω–¥–æ–º
        change = np.random.normal(trend, daily_volatility)
        new_price = prices[-1] * (1 + change)
        prices.append(new_price)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ OHLCV –¥–∞–Ω–Ω—ã—Ö
    data = []
    for i, (date, close) in enumerate(zip(dates, prices)):
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö OHLC
        intraday_vol = np.random.uniform(0.005, 0.02)
        high = close * (1 + np.random.uniform(0, intraday_vol))
        low = close * (1 - np.random.uniform(0, intraday_vol))
        open_price = close * (1 + np.random.uniform(-intraday_vol/2, intraday_vol/2))
        
        # –û–±—ä–µ–º (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏)
        base_volume = 1000
        volume_multiplier = 1 + abs(change) * 10  # –ë–æ–ª—å—à–µ –æ–±—ä–µ–º–∞ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö
        volume = base_volume * volume_multiplier * np.random.uniform(0.5, 1.5)
        
        # ETF –ø–æ—Ç–æ–∫–∏ (–∫–æ—Ä—Ä–µ–ª–∏—Ä—É—é—Ç —Å –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ —Ü–µ–Ω—ã)
        base_flow = 1000000  # $1M –±–∞–∑–æ–≤—ã–π –ø–æ—Ç–æ–∫
        flow_correlation = change * 10000000  # –ü–æ—Ç–æ–∫–∏ —Ä–µ–∞–≥–∏—Ä—É—é—Ç –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã
        etf_flow = base_flow + flow_correlation + np.random.normal(0, 200000)
        
        data.append({
            'timestamp': date,
            'open': round(open_price, 2),
            'high': round(high, 2),
            'low': round(low, 2),
            'close': round(close, 2),
            'volume': round(volume, 0),
            'etf_flow': round(etf_flow, 2)
        })
    
    df = pd.DataFrame(data)
    df.set_index('timestamp', inplace=True)
    
    logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
    logger.info(f"–î–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–Ω: ${df['close'].min():,.2f} - ${df['close'].max():,.2f}")
    logger.info(f"–û–±—â–µ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: {((df['close'].iloc[-1] / df['close'].iloc[0]) - 1):.2%}")
    
    return df


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""
    logger.info("üöÄ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–û–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –ë–ò–¢–ö–û–ò–ù–ê")
    logger.info("=" * 60)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    model_path = os.getenv('MODEL_PATH', 'models/etf_model_v1.pkl')
    lookback_days = int(os.getenv('LOOKBACK_DAYS', '365'))
    
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
        data = create_synthetic_data(lookback_days)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        logger.info("ü§ñ –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        predictor = BitcoinPredictor()
        X, y, feature_columns = predictor.prepare_data(data)
        
        if len(X) < 50:
            logger.error("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        predictor.feature_columns = feature_columns
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        logger.info("üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        metrics = predictor.train(X, y)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        predictor.save_model(model_path)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç—Ä–æ–∫–∏
        prediction = predictor.predict_last_row(X)
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\n" + "="*60)
        print("üéâ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–û–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
        print("="*60)
        print(f"üìä –î–∞–Ω–Ω—ã–µ:")
        print(f"   ‚Ä¢ –ó–∞–ø–∏—Å–µ–π: {len(data)}")
        print(f"   ‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")
        print(f"   ‚Ä¢ –û–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(X)}")
        print(f"   ‚Ä¢ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {y.value_counts().to_dict()}")
        
        print(f"\nüéØ –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏:")
        print(f"   ‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å (Accuracy): {metrics['accuracy']:.4f}")
        print(f"   ‚Ä¢ AUC: {metrics['auc']:.4f}")
        
        print(f"\nüîÆ –ü—Ä–∏–º–µ—Ä –ø—Ä–æ–≥–Ω–æ–∑–∞ (–ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–æ–∫–∞):")
        print(f"   ‚Ä¢ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ä–æ—Å—Ç–∞: {prediction['probability_up']:.4f}")
        print(f"   ‚Ä¢ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {'–†–æ—Å—Ç' if prediction['prediction'] == 1 else '–ü–∞–¥–µ–Ω–∏–µ'}")
        print(f"   ‚Ä¢ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {prediction['confidence']:.4f}")
        
        print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {model_path}")
        
        # –ü–æ–∫–∞–∑–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if hasattr(predictor.model, 'feature_importances_'):
            print(f"\nüìà –¢–û–ü-5 –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í:")
            feature_importance = dict(zip(feature_columns, predictor.model.feature_importances_))
            sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:5]
            
            for i, (feature, importance) in enumerate(sorted_features, 1):
                print(f"   {i}. {feature}: {importance:.4f}")
        
        print("\n" + "="*60)
        print("‚úÖ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–û–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
        print("–¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:")
        print("   python predict_demo.py")
        print("="*60)
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: {e}")
        raise


if __name__ == "__main__":
    main()
